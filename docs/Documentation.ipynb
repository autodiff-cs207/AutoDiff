{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Derivatives are used to solve a large variety of modern-day problems. There are three general methods used to calculate derivatives:\n",
    "1. Symbolic differentiation \n",
    "2. Numerical differentiation\n",
    "3. Automatic differentiation \n",
    "\n",
    "Symbolic differentiation can be very useful, but there are some functions that do not have a symbolic derivative. Additionally, symbolic differentiation can be very costly, as it may recalculate the same expressions many times, or the expression for the derivative may grow exponentially. Sometimes we can avoid these issues by numerically differentiating our function. Often this means using finite differences. The method of finite differences calculates derivative at point $x$ by using the following definition:\n",
    "\n",
    "$$f'(x) = \\lim_{h\\to 0} f(x) \\frac{f(x+h)-f(x)}{h}$$ \n",
    "\n",
    "Finite differences can also be very effective in certain situations. However, as with symbolic differentiation, finite differences has its problems. The biggest issue is that to obtain the most accurate estimate of $f'(x)$, we would like to make $h$ as small as possible; in fact, we would like $h$ to be infinitely small. However, we cannot *actually* make $h$ zero, and thus we must compromise and choose some small-but-not-zero value for $h$, which brings us to our second problem: we cannot precisely represent all numbers. Our machines only have a certain level of precision. When we compute our derivatives numerically we introduce error by approximating values to their closest machine equivalent. To avoid these issues, we turn to our third approach: automatic differentiation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use the package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Please follow these two steps in sequence to install:\n",
    "\n",
    "1. Clone https://github.com/autodiff-cs207/AutoDiff.git\n",
    "2. After cloning, please run:\n",
    "\n",
    "   ` python setup.py install`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "Parent module '' not loaded, cannot perform relative import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-05a7f1b459cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mAutoDiff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mAutoDiff\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMathOps\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: Parent module '' not loaded, cannot perform relative import"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from AutoDiff import DiffObj, Variable, Constant\n",
    "from AutoDiff import MathOps as mo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton Raphson Demo\n",
    "We find one of the roots of the following function using our AutoDiff package:\n",
    "$$\n",
    "f(x) = 5^{\\left(1 + sin\\left(log\\left(5 + x^2\\right)\\right)\\right)} - 10\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable('x')\n",
    "c1 = Constant('c1', 1)\n",
    "c2 = Constant('c2', 2)\n",
    "c3 = Constant('c3', 5)\n",
    "c4 = Constant('c4', 10)\n",
    "f = c3**(c1 + mo.sin(mo.log(c3 + x**c2))) - c4\n",
    "tolerance = 0.001\n",
    "guess = 20\n",
    "max_iter = 10000\n",
    "val_dict = {'x' : guess}\n",
    "evals = []\n",
    "fx = f.get_val(val_dict)\n",
    "for i in range(max_iter):\n",
    "    evals += [fx]\n",
    "    dx = f.get_der(val_dict)['x']\n",
    "    val_dict['x'] = val_dict['x'] - fx/dx\n",
    "    new_fx = f.get_val(val_dict)\n",
    "    if abs(new_fx - fx) < tolerance: fx = new_fx; break\n",
    "    fx = new_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Automatic differentiation (AD) allows us to calculate the derivative to machine precision while avoiding symbolic differentiation's shortcomings. Our package implements on version of AD, the forward mode, by using an extension of the real numbers called the \"dual numbers.\" The forward mode of AD finds the derivative of all intermediate variables with respect to our independent variable and combines them into a final derivative using the chain rule.\n",
    "\n",
    "AD can also be used in \"reverse mode,\" which we will not discuss in detail her, but this method shares many of the same characteristics as forward mode. However, the reverse mode computes derivatives of the dependent variable with respect to the intermediate variables. \n",
    "\n",
    "#### Dual Numbers\n",
    "To carry out the forward mode AD we utilize dual numbers. Dual numbers are defined as numbers of the form $x + x'\\epsilon$, where $\\epsilon^2=0$ and $x \\in \\mathbb{R}^n$. We use operator overloading to redefine elementary operations to suit our problem. To see why this is useful, let's examine how dual numbers behave under different mathematical operations:\n",
    "\n",
    "Addition: $(x+x'\\epsilon) + (y + y'\\epsilon) = x+y + (x'+y')\\epsilon$\n",
    "\n",
    "Subtraction: $(x+x'\\epsilon) - (y + y'\\epsilon) = x-y + (x'-y')\\epsilon$\n",
    "\n",
    "So far, this is as we might expect.\n",
    "\n",
    "Multiplication: $(x+x'\\epsilon) \\times (y + y'\\epsilon) = xy + y(x')\\epsilon+ x(y')\\epsilon$\n",
    "\n",
    "Our definition of $\\epsilon$ allows the multiplication of dual numbers to behave like the product rule.\n",
    "\n",
    "Division: $\\frac{(x+x'\\epsilon)}{(y + y'\\epsilon)} = \\frac{(x+x'\\epsilon)(y - y'\\epsilon)}{(y + y'\\epsilon)(y - y'\\epsilon)} = \\frac{xy+xy'\\epsilon-yx'\\epsilon}{y^2} = \\frac{x}{y}+\\epsilon \\frac{xy'-yx'}{y^2}$\n",
    "\n",
    "Division also follows rules for derivatives.\n",
    "\n",
    "Finally, observe how functions of dual numbers behave:\n",
    "\n",
    "$f(x+x'\\epsilon) = f(x)+\\epsilon f'(x)x'$\n",
    "\n",
    "Which implies the following:\n",
    "\n",
    "$g(f(x+x'\\epsilon)) = g(f(x)+\\epsilon f'(x)x') = g(f(x))+\\epsilon g'(f(x))f'(x)x'$\n",
    "\n",
    "The above example illustrates how dual numbers can be used to simultaneously calculate the value of a function at a point, $g(f(x))$, and the derivative, $g'(f(x))f'(x)x'$.\n",
    "\n",
    "#### Tracing the computational graph\n",
    "By keeping track of the intermediate values of the derivative we can calculate the derivative of composition of many elementary functions. We can picture this decomposition as a graph or table. For example, consider the following function$^{1}$: $$f\\left(x, y, z\\right) = \\dfrac{1}{xyz} + \\sin\\left(\\dfrac{1}{x} + \\dfrac{1}{y} + \\dfrac{1}{z}\\right).$$\n",
    "\n",
    "If we want to evaluate $f$ at $\\left(1, 2, 3\\right)$, we can construct the following table which keeps track for the elementary function, current value, and the elementary function derivative (evaluated with respect to all our variables).\n",
    "\n",
    "| Trace | Elementary Function | Current Value | Elementary Function Derivative | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  | $\\nabla_{z}$ Value  |\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------: | :-----------------: | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$ | $1$ | $\\dot{x}_1$ | $1$ | $0$ | $0$ | \n",
    "| $x_{2}$ | $x_{2}$ | $2$ | $\\dot{x}_2$ | $0$ | $1$ | $0$ | \n",
    "| $x_{3}$ | $x_{3}$ | $3$ | $\\dot{x}_3$ | $0$ | $0$ | $1$ | \n",
    "| $x_{4}$ | $1/x_{1}$ | $1$ | $-\\dot{x}_{1}/x_{1}^{2}$ | $-1$ | $0$ | $0$ | \n",
    "| $x_{5}$ | $1/x_{2}$ | $\\frac{1}{2}$ | $-\\dot{x}_{2}/x_{2}^{2}$ | $0$ | $-\\frac{1}{4}$ | $0$ | \n",
    "| $x_{6}$ | $1/x_{3}$ | $\\frac{1}{3}$ | $-\\dot{x}_{3}/x_{3}^{2}$ | $0$ | $0$ | $-\\frac{1}{9}$ | \n",
    "| $x_{7}$ | $x_4 x_5 x_6$ | $\\frac{1}{6}$ | $x_4(x_5\\dot{x}_6 + x_6\\dot{x}_5) + x_5x_6\\dot{x}_4$ | $-\\frac{1}{6}$ | $-\\frac{1}{12}$ | $-\\frac{1}{18}$ | \n",
    "| $x_{8}$ | $x_4 + x_5 + x_6$ | $\\frac{11}{6}$ | $\\dot{x}_4 + \\dot{x}_5 + \\dot{x}_6$ | $-1$ | $-\\frac{1}{4}$ | $-\\frac{1}{9}$ | \n",
    "| $x_{9}$ | $sin(x_8)$ | $sin(\\frac{11}{6})$ | $cos(x_8)\\dot{x}_8$ | $-cos(\\frac{11}{6})$ | $-\\frac{1}{4}cos(\\frac{11}{6})$ | $-\\frac{1}{9}cos(\\frac{11}{6})$ | \n",
    "| $x_{10}$ | $x_7 + x_9$ | $sin(\\frac{11}{6})+\\frac{1}{6}$ | $\\dot{x}_7 + \\dot{x}_9$ | $-cos(\\frac{11}{6})-\\frac{1}{6}$ | $-\\frac{1}{4}cos(\\frac{11}{6})-\\frac{1}{12}$ | $-\\frac{1}{9}cos(\\frac{11}{6})-\\frac{1}{18}$ | \n",
    "\n",
    "As this example shows, we can use AD for both scalar and vector functions. AD can also be used for vector valued functions. The follow sections will make the implementation of these varients clear.\n",
    "\n",
    "$^1$Example from Harvard CS207 Homework 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directory Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementary Functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
